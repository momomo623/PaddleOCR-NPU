#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PytorchPaddleOCR - ç®€å•æ˜“ç”¨çš„OCRæ¨ç†å°è£…ç±»
åŸºäºpredict_system.pyè¿›è¡Œå°è£…ï¼Œæä¾›ç®€æ´çš„APIæ¥å£
"""

import os
import sys
import cv2
import numpy as np
from typing import List, Dict, Union, Tuple, Optional
from PIL import Image
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
__dir__ = os.path.dirname(os.path.abspath(__file__))
sys.path.append(__dir__)
sys.path.append(os.path.abspath(os.path.join(__dir__, 'tools/infer')))

from tools.infer.predict_system import TextSystem
import tools.infer.pytorchocr_utility as utility


class PytorchPaddleOCR:

    def __init__(
            self,
            use_npu: bool = True,
            npu_device_id: int = 0,
            det_model_path: str = "./models/ptocr_v5_server_det.pth",
            rec_model_path: str = "./models/ptocr_v5_server_rec.pth",
            det_yaml_path: str = "configs/det/PP-OCRv5/PP-OCRv5_server_det.yml",
            rec_yaml_path: str = "configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml",
            rec_char_dict_path: str = "./pytorchocr/utils/dict/ppocrv5_dict.txt",
            rec_image_shape: str = "3,48,320",
            use_angle_cls: bool = True,
            cls_model_path: str = "./models/ch_ptocr_mobile_v2.0_cls_infer.pth",
            drop_score: float = 0.0,
            **kwargs
    ):
        """
        åˆå§‹åŒ–PytorchPaddleOCR

        Args:
            use_npu (bool): æ˜¯å¦ä½¿ç”¨NPU
            npu_device_id (int): NPUè®¾å¤‡ID
            det_model_path (str): æ£€æµ‹æ¨¡å‹è·¯å¾„
            rec_model_path (str): è¯†åˆ«æ¨¡å‹è·¯å¾„
            det_yaml_path (str): æ£€æµ‹æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            rec_yaml_path (str): è¯†åˆ«æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            rec_char_dict_path (str): å­—ç¬¦å­—å…¸è·¯å¾„
            rec_image_shape (str): è¯†åˆ«å›¾åƒå°ºå¯¸
            use_angle_cls (bool): æ˜¯å¦ä½¿ç”¨è§’åº¦åˆ†ç±»
            cls_model_path (str): åˆ†ç±»æ¨¡å‹è·¯å¾„
            drop_score (float): ç½®ä¿¡åº¦é˜ˆå€¼
            **kwargs: å…¶ä»–å‚æ•°
        """

        # åˆ›å»ºå‚æ•°å¯¹è±¡
        self.args = self._create_args(
            use_npu=use_npu,
            npu_device_id=npu_device_id,
            det_model_path=det_model_path,
            rec_model_path=rec_model_path,
            det_yaml_path=det_yaml_path,
            rec_yaml_path=rec_yaml_path,
            rec_char_dict_path=rec_char_dict_path,
            rec_image_shape=rec_image_shape,
            use_angle_cls=use_angle_cls,
            cls_model_path=cls_model_path,
            drop_score=drop_score,
            **kwargs
        )

        # éªŒè¯æ¨¡å‹æ–‡ä»¶å­˜åœ¨
        self._validate_model_files()

        # åˆå§‹åŒ–OCRç³»ç»Ÿ
        try:
            self.text_system = TextSystem(self.args)
            device_type = getattr(self.text_system.text_detector, 'device_type', 'unknown')
            print(f"PytorchPaddleOCR åˆå§‹åŒ–æˆåŠŸ - è®¾å¤‡: {device_type}")
        except Exception as e:
            print(f"PytorchPaddleOCR åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

    def _create_args(self, **kwargs):
        """åˆ›å»ºå‚æ•°å¯¹è±¡"""
        # è·å–é»˜è®¤å‚æ•°
        parser = utility.init_args()
        args = parser.parse_args([])  # ç©ºå‚æ•°åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤å€¼

        # è®¾ç½®å›ºå®šå‚æ•°
        default_params = {
            'use_npu': True,
            'use_gpu': False,  # å›ºå®šä¸ºFalse
            'npu_device_id': 0,
            'det_yaml_path': 'configs/det/PP-OCRv5/PP-OCRv5_server_det.yml',
            'det_model_path': './models/ptocr_v5_server_det.pth',
            'rec_yaml_path': 'configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml',
            'rec_model_path': './models/ptocr_v5_server_rec.pth',
            'rec_char_dict_path': './pytorchocr/utils/dict/ppocrv5_dict.txt',
            'rec_image_shape': '3,48,320',
            'use_angle_cls': True,
            'cls_model_path': './models/ch_ptocr_mobile_v2.0_cls_infer.pth',
            'cls_image_shape': '3,48,192',
            'cls_batch_num': 24,
            'cls_thresh': 0.9,
            'label_list': ['0', '180'],
            'drop_score': 0.5,
            'det_algorithm': 'DB',
            'rec_algorithm': 'SVTR_HGNet',
            'det_limit_side_len': 960,
            'det_box_type': 'quad',
            # åŸå§‹å‚æ•°ï¼ˆå·²æ³¨é‡Šï¼‰
            # 'det_limit_type': 'max',
            # 'det_db_thresh': 0.3,
            # 'det_db_box_thresh': 0.6,
            # 'det_db_unclip_ratio': 1.5,
            # 'use_dilation': False,
            # 'det_db_score_mode': 'fast',

            # 'det_limit_type': 'min',
            # ä¸ºäº†æ£€æµ‹å¤§å›¾ç‰‡
    #         'det_limit_type': 'max',           # ğŸ”‘ å…³é”®æ”¹å˜ï¼šé™åˆ¶æœ€å¤§è¾¹
    #         'det_limit_side_len': 1600,        # ğŸ”‘ æé«˜é™åˆ¶ï¼Œä¿æŒè¶³å¤Ÿç»†èŠ‚
    #         'det_db_thresh': 0.12, 
    #             'det_db_box_thresh': 0.25,         # é€‚åº¦æé«˜ï¼Œå‡å°‘å™ªå£°
    # 'det_db_unclip_ratio': 1.7,        # ç•¥å¾®é™ä½ï¼Œä½†ä¿æŒç®­å¤´å®Œæ•´æ€§
    # 'use_dilation': True,              # ä¿æŒï¼Œå¯¹ç®­å¤´è¯†åˆ«å…³é”®
    # 'det_db_score_mode': 'fast',        # å¤§å›¾åƒæ€§èƒ½ä¼˜åŒ–

            'det_limit_type': 'min',
            # æ–°çš„æ£€æµ‹å‚æ•°é…ç½®
            'det_db_thresh': 0.12,
            'det_db_box_thresh': 0.15,
            'det_db_unclip_ratio': 1.8,
            'use_dilation': True,
            'det_db_score_mode': 'fast',
            'rec_batch_num': 12,
            'max_text_length': 25,
            'use_space_char': True,
            'vis_font_path': os.path.join(__dir__, 'doc/fonts/simfang.ttf'),
            'enable_perf_analysis': False,

            # ğŸ”‘ æŒ‰éœ€æ¸è¿›å¼ç¼©æ”¾é…ç½®
            'prefer_original_size': True,        # å¯ç”¨åŸå§‹ä¼˜å…ˆç­–ç•¥
            'original_size_threshold': 4000000,  # åŸå§‹å›¾åƒå¤„ç†é˜ˆå€¼ï¼ˆ2000Ã—2000åƒç´ ï¼‰
            'enable_pre_resize': True,           # å¯ç”¨æ™ºèƒ½é¢„ç¼©æ”¾ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
            'max_image_pixels': 3000000,         # é¢„ç¼©æ”¾åƒç´ é˜ˆå€¼
            'min_side_after_resize': 1400,       # é¢„ç¼©æ”¾åæœ€å°è¾¹é•¿
            'enable_resize_fallback': True,      # å¯ç”¨å›é€€æœºåˆ¶

            # ğŸ”‘ æ”¹è¿›çš„æ¸è¿›å¼ç¼©æ”¾é…ç½®
            'enable_progressive_resize': True,   # å¯ç”¨æ¸è¿›å¼ç¼©æ”¾
            'progressive_target_range': [800, 1200],  # ç›®æ ‡å°ºå¯¸èŒƒå›´
            'max_progressive_attempts': 5,       # æœ€å¤§å°è¯•æ¬¡æ•°ï¼ˆå¢åŠ åˆ°5æ¬¡ï¼‰
            'ensure_minimum_attempt': True,      # ç¡®ä¿è‡³å°‘å°è¯•æœ€å°å°ºå¯¸
            'aggressive_scaling_after': 2,       # ç¬¬å‡ æ¬¡å¼€å§‹æ¿€è¿›ç¼©æ”¾
            'maintain_aspect_ratio': True,       # ä¸¥æ ¼ä¿æŒå®½é«˜æ¯”
        }

        # æ›´æ–°å‚æ•°
        default_params.update(kwargs)

        # åº”ç”¨å‚æ•°åˆ°argså¯¹è±¡
        for key, value in default_params.items():
            setattr(args, key, value)

        return args

    def _validate_model_files(self):
        """éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        required_files = [
            self.args.det_model_path,
            self.args.rec_model_path,
            self.args.rec_char_dict_path,
        ]

        # å¦‚æœå¯ç”¨è§’åº¦åˆ†ç±»ï¼Œæ·»åŠ clsæ¨¡å‹æ–‡ä»¶æ£€æŸ¥
        if self.args.use_angle_cls:
            required_files.append(self.args.cls_model_path)

        missing_files = []
        for file_path in required_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)

        if missing_files:
            error_msg = f"ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°:\n" + "\n".join(f"  - {f}" for f in missing_files)
            error_msg += "\n\nè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨æ­£ç¡®è·¯å¾„ä¸‹ï¼Œæˆ–ä¸‹è½½ç›¸åº”æ¨¡å‹æ–‡ä»¶ã€‚"
            if self.args.use_angle_cls and self.args.cls_model_path in missing_files:
                error_msg += f"\næç¤º: å¦‚æœä¸éœ€è¦æ–‡æœ¬æ–¹å‘åˆ†ç±»ï¼Œå¯ä»¥åœ¨åˆå§‹åŒ–æ—¶è®¾ç½® use_angle_cls=False"
            raise FileNotFoundError(error_msg)

    def _preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> np.ndarray:
        """
        é¢„å¤„ç†è¾“å…¥å›¾åƒ

        Args:
            image_input: å›¾åƒè¾“å…¥ï¼ˆæ–‡ä»¶è·¯å¾„ã€numpyæ•°ç»„æˆ–PILå›¾åƒï¼‰

        Returns:
            np.ndarray: å¤„ç†åçš„å›¾åƒæ•°ç»„ (BGRæ ¼å¼)
        """
        if isinstance(image_input, str):
            # æ–‡ä»¶è·¯å¾„
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_input}")
            img = cv2.imread(image_input)
            if img is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_input}")

        elif isinstance(image_input, np.ndarray):
            # numpyæ•°ç»„
            img = image_input.copy()
            # å¦‚æœæ˜¯RGBæ ¼å¼ï¼Œè½¬æ¢ä¸ºBGR
            if len(img.shape) == 3 and img.shape[2] == 3:
                # å‡è®¾è¾“å…¥æ˜¯RGBï¼Œè½¬ä¸ºBGR
                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

        elif isinstance(image_input, Image.Image):
            # PILå›¾åƒ
            img = cv2.cvtColor(np.array(image_input), cv2.COLOR_RGB2BGR)

        else:
            raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image_input)}")

        return img

    def ocr(
            self,
            image_input: Union[str, np.ndarray, Image.Image],
            slice_params: Optional[Dict] = None,
            format_output: bool = False
    ) -> Union[List[Dict], Dict]:
        """
        æ‰§è¡ŒOCRè¯†åˆ«

        Args:
            image_input: å›¾åƒè¾“å…¥ï¼ˆæ–‡ä»¶è·¯å¾„ã€numpyæ•°ç»„æˆ–PILå›¾åƒï¼‰
            slice_params: å›¾åƒåˆ‡ç‰‡å‚æ•°ï¼Œç”¨äºå¤„ç†å¤§å›¾åƒ
                {
                    'horizontal_stride': 300,
                    'vertical_stride': 300,
                    'merge_x_thres': 50,
                    'merge_y_thres': 35
                }
            format_output: æ˜¯å¦è¿”å›æ ¼å¼åŒ–çš„å¯è§†åŒ–ç»“æœ

        Returns:
            å¦‚æœformat_output=False: List[Dict]: OCRç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
                {
                    'text': str,           # è¯†åˆ«çš„æ–‡æœ¬
                    'confidence': float,   # ç½®ä¿¡åº¦
                    'bbox': List[List[int]]  # è¾¹ç•Œæ¡†åæ ‡ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                }
            å¦‚æœformat_output=True: Dict: æ ¼å¼åŒ–ç»“æœ
                {
                    'raw_results': List[Dict],     # åŸå§‹è¯†åˆ«ç»“æœ
                    'formatted_text': str,         # æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆæŒ‰è¡Œæ’åˆ—ï¼‰
                    'statistics': Dict             # ç»Ÿè®¡ä¿¡æ¯
                }
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            img = self._preprocess_image(image_input)

            # æ‰§è¡ŒOCRæ¨ç†
            dt_boxes, rec_res, time_dict = self.text_system(
                img,
                cls=self.args.use_angle_cls,
                slice=slice_params or {}
            )

            # æ ¼å¼åŒ–ç»“æœ
            results = []
            if dt_boxes is not None and rec_res is not None:
                for i, (box, (text, confidence)) in enumerate(zip(dt_boxes, rec_res)):
                    result = {
                        'text': text,
                        'confidence': float(confidence),
                        'bbox': box.astype(int).tolist()  # è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
                    }
                    results.append(result)

            # å¦‚æœéœ€è¦æ ¼å¼åŒ–è¾“å‡º
            if format_output:
                return self._format_ocr_results(results, time_dict)
            else:
                return results

        except Exception as e:
            print(f"OCRæ¨ç†å¤±è´¥: {e}")
            raise e

    def _format_ocr_results(self, results: List[Dict], time_dict: Dict) -> Dict:
        """
        æ ¼å¼åŒ–OCRç»“æœä¸ºå¯è§†åŒ–å‹å¥½çš„æ ¼å¼

        Args:
            results: åŸå§‹OCRç»“æœ
            time_dict: å¤„ç†æ—¶é—´å­—å…¸

        Returns:
            Dict: æ ¼å¼åŒ–çš„ç»“æœ
        """
        if not results:
            return {
                'raw_results': [],
                'formatted_text': '',
                'statistics': {
                    'total_text_regions': 0,
                    'total_characters': 0,
                    'avg_confidence': 0.0,
                    'processing_time': time_dict.get('all', 0)
                }
            }

        # è½¬æ¢ results æ ¼å¼ä»¥é€‚åº” _sort_ocr_resultsï¼ŒåŒæ—¶ä¿ç•™ç½®ä¿¡åº¦ä¿¡æ¯
        ocr_data_for_sorting = []
        for res in results:
            ocr_data_for_sorting.append({
                'words': res['text'],
                'confidence': res['confidence'],  # ä¿ç•™ç½®ä¿¡åº¦
                'location': res['bbox']
            })

        # ç”Ÿæˆæ ¼å¼åŒ–æ–‡æœ¬å’Œæ’åºåçš„æ•°æ®å—åˆ—è¡¨
        formatted_text, sorted_words_blocks = self._sort_ocr_results(ocr_data_for_sorting, return_blocks=True)

        # å°†æ’åºåçš„æ•°æ®å—åˆ—è¡¨è½¬æ¢ä¸ºæ‰å¹³åˆ—è¡¨ï¼Œä¿æŒæ’åºé¡ºåº
        # ç›´æ¥æ›¿æ¢ raw_results ä¸ºæ’åºåçš„ç»“æœ
        sorted_raw_results = []
        for line_blocks in sorted_words_blocks:
            for block in line_blocks:
                # è½¬æ¢å›åŸå§‹æ ¼å¼
                sorted_raw_results.append({
                    'text': block['words'],
                    'confidence': block['confidence'],
                    'bbox': block['location']
                })

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŸºäºæ’åºåçš„ç»“æœï¼‰
        total_chars = sum(len(result['text']) for result in sorted_raw_results)
        avg_confidence = sum(result['confidence'] for result in sorted_raw_results) / len(sorted_raw_results) if sorted_raw_results else 0
        total_lines = formatted_text.count('\n') + 1 if formatted_text else 0  # ä»æ ¼å¼åŒ–æ–‡æœ¬è®¡ç®—è¡Œæ•°

        return {
            'raw_results': sorted_raw_results,  # ç›´æ¥è¿”å›æ’åºåçš„ç»“æœ
            'formatted_text': formatted_text,
            'statistics': {
                'total_text_regions': len(sorted_raw_results),
                'total_characters': total_chars,
                'avg_confidence': round(avg_confidence, 3),
                'total_lines': total_lines,
                'processing_time': time_dict.get('all', 0)
            }
        }

    def batch_ocr(
            self,
            image_list: List[Union[str, np.ndarray, Image.Image]],
            show_progress: bool = True,
            format_output: bool = False
    ) -> List[Union[List[Dict], Dict]]:
        """
        æ‰¹é‡OCRè¯†åˆ« (é€å¼ å¤„ç†æ¨¡å¼ - å…¼å®¹æ—§ç‰ˆæœ¬)

        Args:
            image_list: å›¾åƒåˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            format_output: æ˜¯å¦è¿”å›æ ¼å¼åŒ–ç»“æœ

        Returns:
            List: æ¯å¼ å›¾åƒçš„OCRç»“æœåˆ—è¡¨
        """
        results = []
        total = len(image_list)

        for i, image_input in enumerate(image_list):
            if show_progress:
                print(f"å¤„ç†è¿›åº¦: {i + 1}/{total}")

            try:
                result = self.ocr(image_input, format_output=format_output)
                results.append(result)
            except Exception as e:
                print(f"å¤„ç†ç¬¬ {i + 1} å¼ å›¾åƒå¤±è´¥: {e}")
                if format_output:
                    results.append({
                        'raw_results': [],
                        'formatted_text': '',
                        'lines': [],
                        'statistics': {'total_text_regions': 0, 'total_characters': 0, 'avg_confidence': 0.0,
                                       'processing_time': 0}
                    })
                else:
                    results.append([])  # æ·»åŠ ç©ºç»“æœ

        return results

    def batch_ocr_optimized(
            self,
            image_list: List[Union[str, np.ndarray, Image.Image]],
            show_progress: bool = True,
            format_output: bool = False
    ) -> List[Union[List[Dict], Dict]]:
        """
        ä¼˜åŒ–çš„æ‰¹é‡OCRè¯†åˆ« - å……åˆ†åˆ©ç”¨batchæ¨ç†èƒ½åŠ›

        è¯¥å‡½æ•°å°†å¤šå¼ å›¾åƒçš„æ–‡æœ¬æ£€æµ‹ç»“æœåˆå¹¶ï¼Œç„¶åè¿›è¡Œæ‰¹é‡è¯†åˆ«ï¼Œ
        æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨NPUç¯å¢ƒä¸‹ã€‚

        Args:
            image_list: å›¾åƒåˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            format_output: æ˜¯å¦è¿”å›æ ¼å¼åŒ–ç»“æœ

        Returns:
            List: æ¯å¼ å›¾åƒçš„OCRç»“æœåˆ—è¡¨

        Performance:
            ç›¸æ¯”é€å¼ å¤„ç†ï¼Œåœ¨è¯†åˆ«å’Œåˆ†ç±»é˜¶æ®µå¯è·å¾—3-5å€æ€§èƒ½æå‡
        """
        if not image_list:
            return []

        # ä½¿ç”¨å›ºå®šçš„batchå¤§å°
        rec_batch_size = 12  # è¯†åˆ«batchå¤§å°
        cls_batch_size = 24  # åˆ†ç±»batchå¤§å°

        # ä¿å­˜åŸå§‹batché…ç½®
        original_rec_batch = self.args.rec_batch_num
        original_cls_batch = getattr(self.args, 'cls_batch_num', 6)

        try:
            # è®¾ç½®batchå¤§å°
            self.args.rec_batch_num = rec_batch_size
            if hasattr(self.args, 'cls_batch_num'):
                self.args.cls_batch_num = cls_batch_size

            total_images = len(image_list)
            if show_progress:
                print(f"å¼€å§‹ä¼˜åŒ–æ‰¹é‡OCRå¤„ç† (å…±{total_images}å¼ å›¾åƒ)")
                print(
                    f"Batché…ç½®: æ£€æµ‹=1, è¯†åˆ«={rec_batch_size}, åˆ†ç±»={cls_batch_size}")

            # Step 1: é¢„å¤„ç†æ‰€æœ‰å›¾åƒ
            if show_progress:
                print("Step 1: é¢„å¤„ç†å›¾åƒ...")

            processed_images = []
            valid_indices = []

            for i, image_input in enumerate(image_list):
                try:
                    img = self._preprocess_image(image_input)
                    processed_images.append(img)
                    valid_indices.append(i)
                except Exception as e:
                    if show_progress:
                        print(f"å›¾åƒ {i + 1} é¢„å¤„ç†å¤±è´¥: {e}")

            if not processed_images:
                empty_result = {
                    'raw_results': [],
                    'formatted_text': '',
                    'lines': [],
                    'statistics': {'total_text_regions': 0, 'total_characters': 0, 'avg_confidence': 0.0,
                                   'processing_time': 0}
                } if format_output else []
                return [empty_result for _ in image_list]

            # Step 2: æ‰¹é‡æ£€æµ‹æ–‡æœ¬åŒºåŸŸ
            if show_progress:
                print("Step 2: æ‰¹é‡æ–‡æœ¬æ£€æµ‹...")

            all_dt_boxes = []
            all_img_crops = []  # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬åŒºåŸŸå›¾åƒ
            image_crop_counts = []  # æ¯å¼ å›¾åƒçš„æ–‡æœ¬åŒºåŸŸæ•°é‡

            for i, img in enumerate(processed_images):
                if show_progress and (i + 1) % max(1, len(processed_images) // 10) == 0:
                    print(f"   æ£€æµ‹è¿›åº¦: {i + 1}/{len(processed_images)}")

                # æ£€æµ‹æ–‡æœ¬åŒºåŸŸ
                dt_boxes, _ = self.text_system.text_detector(img)
                all_dt_boxes.append(dt_boxes)

                # æå–æ–‡æœ¬åŒºåŸŸå›¾åƒ
                if dt_boxes is not None and len(dt_boxes) > 0:
                    img_crops = []
                    for box in dt_boxes:
                        # ä»åŸå›¾ä¸­è£å‰ªæ–‡æœ¬åŒºåŸŸ
                        crop_img = self._crop_image(img, box)
                        if crop_img is not None:
                            img_crops.append(crop_img)

                    all_img_crops.extend(img_crops)
                    image_crop_counts.append(len(img_crops))
                else:
                    image_crop_counts.append(0)

            if not all_img_crops:
                empty_result = {
                    'raw_results': [],
                    'formatted_text': '',
                    'lines': [],
                    'statistics': {'total_text_regions': 0, 'total_characters': 0, 'avg_confidence': 0.0,
                                   'processing_time': 0}
                } if format_output else []
                return [empty_result for _ in image_list]

            # Step 3: æ‰¹é‡è§’åº¦åˆ†ç±» (å¦‚æœå¯ç”¨)
            cls_res_all = None
            if self.args.use_angle_cls:
                if show_progress:
                    print(f"Step 3: æ‰¹é‡è§’åº¦åˆ†ç±» ({len(all_img_crops)}ä¸ªæ–‡æœ¬åŒºåŸŸ)...")

                all_img_crops, cls_res_all, _ = self.text_system.text_classifier(all_img_crops)

            # Step 4: æ‰¹é‡æ–‡æœ¬è¯†åˆ«
            if show_progress:
                print(f"Step 4: æ‰¹é‡æ–‡æœ¬è¯†åˆ« ({len(all_img_crops)}ä¸ªæ–‡æœ¬åŒºåŸŸ)...")

            rec_res_all, _ = self.text_system.text_recognizer(all_img_crops)

            # Step 5: ç»„è£…ç»“æœ
            if show_progress:
                print("Step 5: ç»„è£…ç»“æœ...")

            results = []
            crop_idx = 0

            for img_idx in range(len(processed_images)):
                img_results = []
                crop_count = image_crop_counts[img_idx]

                if crop_count > 0:
                    img_dt_boxes = all_dt_boxes[img_idx]

                    for box_idx in range(crop_count):
                        if crop_idx < len(rec_res_all):
                            text, confidence = rec_res_all[crop_idx]

                            # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
                            if confidence >= self.args.drop_score:
                                result = {
                                    'text': text,
                                    'confidence': float(confidence),
                                    'bbox': img_dt_boxes[box_idx].astype(int).tolist()
                                }
                                img_results.append(result)

                        crop_idx += 1

                # å¦‚æœéœ€è¦æ ¼å¼åŒ–è¾“å‡º
                if format_output:
                    formatted_result = self._format_ocr_results(img_results, {'all': 0})
                    results.append(formatted_result)
                else:
                    results.append(img_results)

            # ä¸ºæ— æ•ˆå›¾åƒè¡¥å……ç©ºç»“æœ
            final_results = []
            valid_idx = 0
            for i in range(total_images):
                if i in valid_indices:
                    final_results.append(results[valid_idx])
                    valid_idx += 1
                else:
                    if format_output:
                        final_results.append({
                            'raw_results': [],
                            'formatted_text': '',
                            'lines': [],
                            'statistics': {'total_text_regions': 0, 'total_characters': 0, 'avg_confidence': 0.0,
                                           'processing_time': 0}
                        })
                    else:
                        final_results.append([])

            if show_progress:
                if format_output:
                    total_text_regions = sum(len(r['raw_results']) for r in final_results)
                else:
                    total_text_regions = sum(len(r) for r in final_results)
                print(f"æ‰¹é‡OCRå®Œæˆ! å…±å¤„ç†{total_images}å¼ å›¾åƒï¼Œè¯†åˆ«{total_text_regions}ä¸ªæ–‡æœ¬åŒºåŸŸ")

            return final_results

        except Exception as e:
            print(f"æ‰¹é‡OCRå¤„ç†å¤±è´¥: {e}")
            # é™çº§åˆ°é€å¼ å¤„ç†
            if show_progress:
                print("é™çº§åˆ°é€å¼ å¤„ç†æ¨¡å¼...")
            return self.batch_ocr(image_list, show_progress, format_output)

        finally:
            # æ¢å¤åŸå§‹batché…ç½®
            self.args.rec_batch_num = original_rec_batch
            if hasattr(self.args, 'cls_batch_num'):
                self.args.cls_batch_num = original_cls_batch

    def _crop_image(self, img: np.ndarray, box: np.ndarray) -> Optional[np.ndarray]:
        """
        ä»å›¾åƒä¸­è£å‰ªæ–‡æœ¬åŒºåŸŸ

        Args:
            img: åŸå§‹å›¾åƒ
            box: æ–‡æœ¬æ¡†åæ ‡ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]

        Returns:
            è£å‰ªåçš„å›¾åƒï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # è·å–è¾¹ç•Œæ¡†
            box = box.astype(np.int32)

            # è®¡ç®—æœ€å°å¤–æ¥çŸ©å½¢
            x_min = np.min(box[:, 0])
            x_max = np.max(box[:, 0])
            y_min = np.min(box[:, 1])
            y_max = np.max(box[:, 1])

            # è¾¹ç•Œæ£€æŸ¥
            h, w = img.shape[:2]
            x_min = max(0, x_min)
            y_min = max(0, y_min)
            x_max = min(w, x_max)
            y_max = min(h, y_max)

            if x_max <= x_min or y_max <= y_min:
                return None

            # è£å‰ªå›¾åƒ
            crop_img = img[y_min:y_max, x_min:x_max]

            if crop_img.size == 0:
                return None

            return crop_img

        except Exception as e:
            print(f"è£å‰ªå›¾åƒå¤±è´¥: {e}")
            return None

    def _sort_ocr_results(self, ocr_data: List[Dict], return_blocks: bool = False) -> Union[str, Tuple[str, List[List[Dict]]]]:
        """
        å¯¹OCRè¯†åˆ«ç»“æœè¿›è¡Œè¡Œåˆ—æ’åºï¼Œä»¥è¿˜åŸå›¾ç‰‡ä¸­çš„æ–‡æœ¬å¸ƒå±€ã€‚

        ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬ï¼šä¿®å¤äº†åŸå§‹ç®—æ³•ä¸­çš„è¡Œåˆ†ç»„é—®é¢˜
        - åŸºäºä¸¥æ ¼çš„Yåæ ‡é‡å åˆ¤æ–­ï¼Œé¿å…ç´¯ç§¯æ•ˆåº”
        - ä½¿ç”¨é€å¯¹æ¯”è¾ƒè€Œéæ•´è¡ŒèŒƒå›´æ¯”è¾ƒ
        - æ›´ç²¾ç¡®çš„é‡å æ¯”ä¾‹å’Œä¸­å¿ƒè·ç¦»åˆ¤æ–­

        Args:
            ocr_data (list): ä¸€ä¸ªåŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªè¯†åˆ«å‡ºçš„æ–‡æœ¬å—ï¼Œ
                             éœ€è¦åŒ…å« "words" (æ–‡æœ¬å†…å®¹) å’Œ "location" (è¾¹ç•Œæ¡†åæ ‡)ã€‚
                             "location" æ˜¯ä¸€ä¸ªåŒ…å«å››ä¸ª[x, y]åæ ‡çš„åˆ—è¡¨ï¼Œ
                             ä»£è¡¨å·¦ä¸Šã€å³ä¸Šã€å³ä¸‹ã€å·¦ä¸‹å››ä¸ªé¡¶ç‚¹ã€‚
            return_blocks (bool): æ˜¯å¦åŒæ—¶è¿”å›æ’åºåçš„æ•°æ®å—åˆ—è¡¨ï¼Œé»˜è®¤False

        Returns:
            Union[str, Tuple[str, List[List[Dict]]]]:
                - å½“ return_blocks=False æ—¶ï¼Œè¿”å›æ’åºåçš„markdownæ ¼å¼å­—ç¬¦ä¸²
                - å½“ return_blocks=True æ—¶ï¼Œè¿”å›å…ƒç»„ (markdown_result, sorted_words_block_list)
                  å…¶ä¸­ sorted_words_block_list æ˜¯æŒ‰è¡Œåˆ†ç»„çš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not ocr_data:
            if return_blocks:
                return "", []
            return ""

        # 1. é¢„å¤„ç†ï¼šåˆ›å»ºæ•°æ®å‰¯æœ¬å¹¶è®¡ç®—æ¯ä¸ªæ–‡æœ¬å—çš„Yåæ ‡ä¿¡æ¯
        processed_boxes = []
        for box in ocr_data:
            # åˆ›å»ºåŸå§‹æ•°æ®çš„å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            box_copy = box.copy()
            y_top = box_copy['location'][0][1]
            y_bottom = box_copy['location'][3][1]
            box_copy['y_top'] = y_top
            box_copy['y_bottom'] = y_bottom
            box_copy['y_center'] = (y_top + y_bottom) / 2
            box_copy['height'] = y_bottom - y_top
            processed_boxes.append(box_copy)

        # 2. æŒ‰Yä¸­å¿ƒåæ ‡æ’åº
        sorted_boxes = sorted(processed_boxes, key=lambda x: x['y_center'])

        # 3. åŸºäºä¸¥æ ¼çš„Yåæ ‡é‡å åˆ¤æ–­è¿›è¡Œåˆ†ç»„
        lines = []
        used_indices = set()

        for i, current_box in enumerate(sorted_boxes):
            if i in used_indices:
                continue

            current_line = [current_box]
            used_indices.add(i)

            # æŸ¥æ‰¾ä¸å½“å‰æ–‡æœ¬å—åœ¨åŒä¸€è¡Œçš„å…¶ä»–æ–‡æœ¬å—
            for j, other_box in enumerate(sorted_boxes):
                if j in used_indices or j <= i:
                    continue

                # è®¡ç®—Yåæ ‡é‡å 
                overlap_top = max(current_box['y_top'], other_box['y_top'])
                overlap_bottom = min(current_box['y_bottom'], other_box['y_bottom'])
                overlap_height = max(0, overlap_bottom - overlap_top)

                # è®¡ç®—é‡å æ¯”ä¾‹ï¼ˆç›¸å¯¹äºä¸¤ä¸ªæ–‡æœ¬å—çš„è¾ƒå°é«˜åº¦ï¼‰
                min_height = min(current_box['height'], other_box['height'])
                overlap_ratio = overlap_height / min_height if min_height > 0 else 0

                # ä¸­å¿ƒç‚¹è·ç¦»
                center_distance = abs(current_box['y_center'] - other_box['y_center'])

                # ä¸¥æ ¼çš„åŒè¡Œåˆ¤æ–­æ¡ä»¶ï¼š
                # 1. é‡å æ¯”ä¾‹ >= 0.7 (éå¸¸ä¸¥æ ¼çš„é‡å è¦æ±‚)
                # 2. æˆ–è€…ä¸­å¿ƒè·ç¦» <= è¾ƒå°é«˜åº¦çš„ä¸€åŠ
                max_center_distance = min_height / 2

                if overlap_ratio >= 0.7 or center_distance <= max_center_distance:
                    current_line.append(other_box)
                    used_indices.add(j)

            lines.append(current_line)

        # 4. è¡Œå†…æ’åºå’Œæ ¼å¼åŒ–è¾“å‡º
        output_lines = []
        sorted_lines = []  # å­˜å‚¨æ’åºåçš„æ•°æ®å—åˆ—è¡¨

        for line in lines:
            # å¯¹æ¯ä¸€è¡Œå†…çš„æ–‡æœ¬æ¡†æ ¹æ®xåæ ‡ä»å·¦åˆ°å³æ’åº
            sorted_line = sorted(line, key=lambda x: x['location'][0][0])

            # å¦‚æœéœ€è¦è¿”å›æ•°æ®å—ï¼Œæ¸…ç†ä¸´æ—¶å­—æ®µå¹¶åˆ›å»ºå¹²å‡€çš„å‰¯æœ¬
            if return_blocks:
                clean_line = []
                for box in sorted_line:
                    # åˆ›å»ºä¸åŒ…å«ä¸´æ—¶å­—æ®µçš„å¹²å‡€å‰¯æœ¬
                    clean_box = {k: v for k, v in box.items()
                               if k not in ['y_top', 'y_bottom', 'y_center', 'height']}
                    clean_line.append(clean_box)
                sorted_lines.append(clean_line)

            # å°†è¡Œå†…æ‰€æœ‰æ–‡æœ¬æ¡†çš„æ–‡å­—ç”¨ç©ºæ ¼è¿æ¥èµ·æ¥
            line_text = " ".join([box['words'] for box in sorted_line])
            output_lines.append(line_text)

        # 5. å°†æ‰€æœ‰è¡Œç”¨æ¢è¡Œç¬¦è¿æ¥æˆæœ€ç»ˆçš„è¾“å‡ºå­—ç¬¦ä¸²
        markdown_result = "\n".join(output_lines)

        if return_blocks:
            return markdown_result, sorted_lines
        return markdown_result

    def sort_ocr_results_with_blocks(self, ocr_data: List[Dict]) -> Tuple[str, List[List[Dict]]]:
        """
        å¯¹OCRè¯†åˆ«ç»“æœè¿›è¡Œè¡Œåˆ—æ’åºï¼ŒåŒæ—¶è¿”å›markdownæ–‡æœ¬å’Œæ’åºåçš„æ•°æ®å—åˆ—è¡¨ã€‚

        è¿™æ˜¯ _sort_ocr_results çš„ä¾¿æ·åŒ…è£…æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºéœ€è¦åŒæ—¶è·å–æ ¼å¼åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®çš„åœºæ™¯ã€‚

        Args:
            ocr_data (list): ä¸€ä¸ªåŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªè¯†åˆ«å‡ºçš„æ–‡æœ¬å—ï¼Œ
                             éœ€è¦åŒ…å« "words" (æ–‡æœ¬å†…å®¹) å’Œ "location" (è¾¹ç•Œæ¡†åæ ‡)ã€‚

        Returns:
            Tuple[str, List[List[Dict]]]:
                - ç¬¬ä¸€ä¸ªå…ƒç´ ï¼šæ’åºåçš„markdownæ ¼å¼å­—ç¬¦ä¸²ï¼ŒåŒä¸€è¡Œçš„æ–‡æœ¬ç”¨ç©ºæ ¼éš”å¼€ï¼Œä¸åŒè¡Œç”¨æ¢è¡Œç¬¦éš”å¼€
                - ç¬¬äºŒä¸ªå…ƒç´ ï¼šæŒ‰è¡Œåˆ†ç»„çš„æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªåŒ…å«è¯¥è¡Œæ‰€æœ‰æ–‡æœ¬å—çš„åˆ—è¡¨
                  æ¯ä¸ªæ–‡æœ¬å—ä¿æŒåŸæœ‰ç»“æ„ï¼ˆåŒ…å«wordsã€confidenceã€locationç­‰å­—æ®µï¼‰

        Example:
            >>> ocr_data = [{'words': 'æ–‡æœ¬1', 'location': [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]}, ...]
            >>> markdown_text, sorted_blocks = ocr.sort_ocr_results_with_blocks(ocr_data)
            >>> print(markdown_text)  # æ ¼å¼åŒ–çš„æ–‡æœ¬ç»“æœ
            >>> print(len(sorted_blocks))  # è¡Œæ•°
            >>> print(sorted_blocks[0])  # ç¬¬ä¸€è¡Œçš„æ‰€æœ‰æ–‡æœ¬å—
        """
        return self._sort_ocr_results(ocr_data, return_blocks=True)

    def get_text_only(self, image_input: Union[str, np.ndarray, Image.Image]) -> List[str]:
        """
        åªè·å–æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…å«åæ ‡å’Œç½®ä¿¡åº¦

        Args:
            image_input: å›¾åƒè¾“å…¥

        Returns:
            List[str]: æ–‡æœ¬åˆ—è¡¨
        """
        results = self.ocr(image_input, format_output=False)
        return [result['text'] for result in results]

    def get_full_text(self, image_input: Union[str, np.ndarray, Image.Image], separator: str = '\n') -> str:
        """
        è·å–å®Œæ•´æ–‡æœ¬ï¼Œå°†æ‰€æœ‰è¯†åˆ«ç»“æœè¿æ¥ä¸ºå•ä¸ªå­—ç¬¦ä¸²

        Args:
            image_input: å›¾åƒè¾“å…¥
            separator: æ–‡æœ¬åˆ†éš”ç¬¦

        Returns:
            str: å®Œæ•´æ–‡æœ¬
        """
        texts = self.get_text_only(image_input)
        return separator.join(texts)

    def get_formatted_result(self, image_input: Union[str, np.ndarray, Image.Image]) -> Dict:
        """
        è·å–æ ¼å¼åŒ–çš„OCRç»“æœï¼ŒåŒ…å«åˆ†è¡Œä¿¡æ¯

        Args:
            image_input: å›¾åƒè¾“å…¥

        Returns:
            Dict: æ ¼å¼åŒ–çš„OCRç»“æœ
        """
        return self.ocr(image_input, format_output=True)

    def __call__(self, image_input: Union[str, np.ndarray, Image.Image]) -> List[Dict]:
        """
        æ”¯æŒç›´æ¥è°ƒç”¨ï¼Œè¿”å›æ ‡å‡†OCRç»“æœ

        Args:
            image_input: å›¾åƒè¾“å…¥

        Returns:
            List[Dict]: OCRç»“æœ
        """
        return self.ocr(image_input, format_output=False)


# ä¾¿æ·å‡½æ•°
def create_ocr(
        use_npu: bool = True,
        use_angle_cls: bool = True,
        **kwargs
) -> PytorchPaddleOCR:
    """
    åˆ›å»ºOCRå®ä¾‹çš„ä¾¿æ·å‡½æ•°

    Args:
        use_npu: æ˜¯å¦ä½¿ç”¨NPU
        use_angle_cls: æ˜¯å¦ä½¿ç”¨è§’åº¦åˆ†ç±»
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        PytorchPaddleOCR: OCRå®ä¾‹
    """
    return PytorchPaddleOCR(use_npu=use_npu, use_angle_cls=use_angle_cls, **kwargs)

# if __name__ == "__main__":
#     # ä½¿ç”¨ç¤ºä¾‹
#     print("PytorchPaddleOCR æ‰¹é‡å¤„ç†æ¼”ç¤º")
#     print("=" * 60)
#
#     # åˆ›å»ºOCRå®ä¾‹
#     try:
#         ocr = create_ocr(use_npu=True, use_angle_cls=True)
#         print("OCRå®ä¾‹åˆ›å»ºæˆåŠŸ")
#         print(f"é…ç½®: NPU=True, æ–‡æœ¬æ–¹å‘åˆ†ç±»=True")
#
#         # æ‰«æå›¾åƒæ–‡ä»¶å¤¹
#         images_folder = "./doc/imgs"
#         image_list = []
#
#         if os.path.exists(images_folder):
#             print(f"\næ‰«æå›¾åƒæ–‡ä»¶å¤¹: {images_folder}")
#
#             # æ”¯æŒçš„å›¾åƒæ ¼å¼
#             supported_formats = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')
#
#             for filename in os.listdir(images_folder):
#                 if filename.lower().endswith(supported_formats):
#                     image_path = os.path.join(images_folder, filename)
#                     image_list.append(image_path)
#
#             # æ’åºæ–‡ä»¶åˆ—è¡¨
#             image_list.sort()
#
#             if image_list:
#                 print(f"å‘ç° {len(image_list)} å¼ å›¾åƒæ–‡ä»¶:")
#                 for i, img_path in enumerate(image_list):
#                     print(f"  {i + 1}. {os.path.basename(img_path)}")
#
#                 # å•å¼ å›¾åƒæµ‹è¯•ï¼ˆç¬¬ä¸€å¼ ï¼‰
#                 print(f"\n" + "=" * 50)
#                 print("å•å¼ å›¾åƒæµ‹è¯•")
#                 print("=" * 50)
#
#                 test_image = image_list[0]
#                 print(f"æµ‹è¯•å›¾åƒ: {os.path.basename(test_image)}")
#
#                 # æ‰§è¡Œæ ‡å‡†OCR
#                 print("\næ ‡å‡†OCRç»“æœ:")
#                 results = ocr(test_image)
#                 print(f"è¯†åˆ«åˆ° {len(results)} ä¸ªæ–‡æœ¬åŒºåŸŸ:")
#                 for i, result in enumerate(results):
#                     print(f"{i + 1}. æ–‡æœ¬: '{result['text']}'")
#                     print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
#                     # ç®€åŒ–åæ ‡æ˜¾ç¤º
#                     bbox = result['bbox']
#                     print(f"   åŒºåŸŸ: ({bbox[0][0]},{bbox[0][1]}) - ({bbox[2][0]},{bbox[2][1]})")
#
#                 # æ‰§è¡Œæ ¼å¼åŒ–OCR
#                 print("\næ ¼å¼åŒ–OCRç»“æœ:")
#                 formatted_result = ocr.get_formatted_result(test_image)
#                 print(f"ç»Ÿè®¡ä¿¡æ¯: {formatted_result['statistics']}")
#                 print(f"\næŒ‰è¡Œç»„ç»‡çš„æ–‡æœ¬ ({len(formatted_result['lines'])} è¡Œ):")
#                 for i, line in enumerate(formatted_result['lines']):
#                     print(f"ç¬¬{i + 1}è¡Œ: '{line['text']}'")
#                     print(f"  ç½®ä¿¡åº¦: {line['confidence']:.3f}, å•è¯æ•°: {line['word_count']}")
#
#                 print(f"\nå®Œæ•´æ ¼å¼åŒ–æ–‡æœ¬:")
#                 print(formatted_result['formatted_text'])
#
#                 # è·å–çº¯æ–‡æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
#                 full_text = ocr.get_full_text(test_image)
#                 print(f"\nçº¯æ–‡æœ¬è¾“å‡º:")
#                 print(full_text)
#
#                 # æ‰¹é‡å¤„ç†æ¼”ç¤º
#                 if len(image_list) > 1:
#                     print(f"\n" + "=" * 60)
#                     print("æ‰¹é‡OCRå¤„ç†æ€§èƒ½å¯¹æ¯”")
#                     print("=" * 60)
#
#                     import time
#
#                     print(f"\nå‡†å¤‡å¤„ç† {len(image_list)} å¼ å›¾åƒ...")
#
#                     # æ–¹æ³•1: ä¼ ç»Ÿæ‰¹é‡å¤„ç†
#                     print("\næ–¹æ³•1: ä¼ ç»Ÿæ‰¹é‡å¤„ç† (é€å¼ )")
#                     start_time = time.time()
#                     results_traditional = ocr.batch_ocr(image_list, show_progress=True)
#                     traditional_time = time.time() - start_time
#                     print(f"ä¼ ç»Ÿæ–¹æ³•è€—æ—¶: {traditional_time:.3f}ç§’")
#
#                     # æ–¹æ³•2: ä¼˜åŒ–æ‰¹é‡å¤„ç†
#                     print("\næ–¹æ³•2: ä¼˜åŒ–æ‰¹é‡å¤„ç† (çœŸæ­£batch)")
#                     start_time = time.time()
#                     results_optimized = ocr.batch_ocr_optimized(
#                         image_list,
#                         show_progress=True,
#                         format_output=True
#                     )
#                     optimized_time = time.time() - start_time
#                     print(f"ä¼˜åŒ–æ–¹æ³•è€—æ—¶: {optimized_time:.3f}ç§’")
#
#                     # æ–¹æ³•3: æ ¼å¼åŒ–æ‰¹é‡å¤„ç†æ¼”ç¤º
#                     print("\næ–¹æ³•3: æ ¼å¼åŒ–æ‰¹é‡å¤„ç†æ¼”ç¤º")
#                     start_time = time.time()
#                     formatted_results = ocr.batch_ocr_optimized(
#                         image_list[:3],  # åªå¤„ç†å‰3å¼ å›¾åƒä½œä¸ºæ¼”ç¤º
#                         show_progress=True,
#                         format_output=True
#                     )
#                     formatted_time = time.time() - start_time
#                     print(f"æ ¼å¼åŒ–æ–¹æ³•è€—æ—¶: {formatted_time:.3f}ç§’")
#
#                     print("\næ ¼å¼åŒ–ç»“æœç¤ºä¾‹:")
#                     for i, result in enumerate(formatted_results[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªç»“æœ
#                         filename = os.path.basename(image_list[i])
#                         stats = result['statistics']
#                         print(f"\nå›¾åƒ {i + 1}: {filename}")
#                         print(
#                             f"  ç»Ÿè®¡: {stats['total_text_regions']}ä¸ªåŒºåŸŸ, {stats['total_lines']}è¡Œ, {stats['total_characters']}ä¸ªå­—ç¬¦")
#                         print(f"  å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}")
#                         print(f"  æ ¼å¼åŒ–æ–‡æœ¬é¢„è§ˆ:")
#                         preview_text = result['formatted_text'][:100] + '...' if len(
#                             result['formatted_text']) > 100 else result['formatted_text']
#                         print(f"    {preview_text}")
#
#                     # æ€§èƒ½å¯¹æ¯”
#                     if traditional_time > 0 and optimized_time > 0:
#                         speedup = traditional_time / optimized_time
#                         print(f"\næ€§èƒ½å¯¹æ¯”:")
#                         print(f"ä¼ ç»Ÿæ–¹æ³•: {traditional_time:.3f}ç§’ ({len(image_list) / traditional_time:.2f} å›¾ç‰‡/ç§’)")
#                         print(f"ä¼˜åŒ–æ–¹æ³•: {optimized_time:.3f}ç§’ ({len(image_list) / optimized_time:.2f} å›¾ç‰‡/ç§’)")
#                         print(f"æ€§èƒ½æå‡: {speedup:.2f}x åŠ é€Ÿ")
#
#                     # éªŒè¯ç»“æœä¸€è‡´æ€§
#                     total_texts_traditional = sum(len(r) for r in results_traditional)
#                     total_texts_optimized = sum(len(r) for r in results_optimized)
#
#                     print(f"\nç»“æœéªŒè¯:")
#                     print(f"ä¼ ç»Ÿæ–¹æ³•è¯†åˆ«æ–‡æœ¬æ•°: {total_texts_traditional}")
#                     print(f"ä¼˜åŒ–æ–¹æ³•è¯†åˆ«æ–‡æœ¬æ•°: {total_texts_optimized}")
#
#                     if total_texts_traditional == total_texts_optimized:
#                         print("ç»“æœä¸€è‡´æ€§éªŒè¯é€šè¿‡")
#                     else:
#                         print("ç»“æœå­˜åœ¨å·®å¼‚ï¼Œè¯·æ£€æŸ¥")
#
#                     # è¯¦ç»†ç»“æœå±•ç¤º
#                     print(f"\nå„å›¾åƒè¯†åˆ«ç»“æœ:")
#                     for i, (img_path, img_results) in enumerate(zip(image_list, results_optimized)):
#                         filename = os.path.basename(img_path)
#                         text_count = len(img_results)
#                         print(f"{i + 1:2d}. {filename:<20} - {text_count:2d} ä¸ªæ–‡æœ¬åŒºåŸŸ")
#
#                         # æ˜¾ç¤ºå‰3ä¸ªè¯†åˆ«ç»“æœ
#                         if img_results:
#                             texts = [r['text'][:20] + '...' if len(r['text']) > 20 else r['text']
#                                      for r in img_results[:3]]
#                             print(f"     æ–‡æœ¬é¢„è§ˆ: {texts}")
#                             if len(img_results) > 3:
#                                 print(f"     ... è¿˜æœ‰ {len(img_results) - 3} ä¸ªæ–‡æœ¬åŒºåŸŸ")
#
#                 else:
#                     print(f"\nåªæ‰¾åˆ°1å¼ å›¾åƒï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡å¤„ç†æ€§èƒ½å¯¹æ¯”")
#                     print("å»ºè®®åœ¨ ./doc/imgs æ–‡ä»¶å¤¹ä¸­æ”¾å…¥æ›´å¤šå›¾åƒæ–‡ä»¶")
#
#             else:
#                 print("æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å›¾åƒæ–‡ä»¶")
#                 print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats)}")
#
#         else:
#             print(f"å›¾åƒæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {images_folder}")
#             print("è¯·åˆ›å»ºæ–‡ä»¶å¤¹å¹¶æ”¾å…¥å›¾åƒæ–‡ä»¶")
#
#         print(f"\n" + "=" * 60)
#         print("ä½¿ç”¨è¯´æ˜")
#         print("=" * 60)
#         print("1. å°†å›¾åƒæ–‡ä»¶æ”¾å…¥ ./doc/imgs æ–‡ä»¶å¤¹")
#         print("2. æ”¯æŒæ ¼å¼: JPG, JPEG, PNG, ")
#         print("3. ç¨‹åºä¼šè‡ªåŠ¨å¯¹æ¯”ä¸¤ç§æ‰¹é‡å¤„ç†æ–¹æ³•çš„æ€§èƒ½")
#         print("4. å¯¹äºå¤§é‡å›¾åƒï¼Œå»ºè®®ä½¿ç”¨ batch_ocr_optimized() æ–¹æ³•")
#
#
#     except Exception as e:
#         print(f"ç¨‹åºè¿è¡Œå¤±è´¥: {e}")